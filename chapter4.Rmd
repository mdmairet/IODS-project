---
title: "Chapter 4: Clustering and classification"
author: "Marc Denojean-Mairet"
date: "27/11/2021"
output: html_document
---

# Chapter 4: Clustering and Classification

## Part 2: explore the structure and the dimensions of the data

Loading the Boston data.


```{r message = FALSE, warning = FALSE}

# set plots size
knitr::opts_chunk$set(fig.width=16, fig.height=10) 


#code from DataCamp.

#access the MASS package
library (dplyr)
library(MASS)
library(corrplot)
library(tidyr)


#load the data
data("Boston")

#explore the dataset
str(Boston)
dim(Boston)
summary(Boston)
```
## Description:
The Boston data set represent the housing values in Suburbs of Boston.
<br><br>
The Boston data frame has **506** rows and **14** columns.
<br><br>
This data frame contains the following columns:
<br><br>
**crim** = per capita crime rate by town<br>
**zn** = proportion of residential land zoned for lots over 25,000 sq.ft<br>
**indus** = proportion of non-retail business acres per town<br>
**chas** = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)<br>
**nox** = nitrogen oxides concentration (parts per 10 million)<br>
**rm** = average number of rooms per dwelling<br>
**age** = proportion of owner-occupied units built prior to 1940<br>
**dis** = weighted mean of distances to five Boston employment centres<br>
**rad** = index of accessibility to radial highways<br>
**tax** = full-value property-tax rate per $10,000<br>
**ptratio** = pupil-teacher ratio by town<br>
**black** = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<br>
**lstat** = lower status of the population (percent)<br>
**medv** = median value of owner-occupied homes in \$1000s<br>

## Source:

Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.

Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley. 

## Part 3: graphical overview of the data 

```{r message = FALSE, warning = FALSE}

#plot matrix of the variables
pairs(Boston, gap=1/30)

# MASS, corrplot, tidyr and Boston dataset are available

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

corrplot(cor_matrix, method = 'number', type="upper")

```

```{r message = FALSE, warning = FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

```{r message = FALSE, warning = FALSE}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

```{r message = FALSE, warning = FALSE}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "orange", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```


```{r message = FALSE, warning = FALSE}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

```{r message = FALSE, warning = FALSE}
# load MASS and Boston
library(MASS)
data('Boston')

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = 'manhattan')

# look at the summary of the distances
summary(dist_man)
```

```{r message = FALSE, warning = FALSE}
# k-means clustering
km <-kmeans(Boston, centers = 3)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```

```{r message = FALSE, warning = FALSE}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)


#install.packages("plotly")
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers')
```

